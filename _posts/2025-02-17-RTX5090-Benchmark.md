---
layout: post
title: Benchmarking Nvidia RTX 5090 
title_img: /assets/post25/title.webp
abstract: >
    Benchmarking the top consumer-grade Nvidia Blackwell GPU against previous generations in Computer Vision and LLM applications.
date: 2025-02-17 23:00:00 +0000
categories: Hardware DeepLearning Benchmark
article: true
sitemap:
    lastmod: 2025-03-24
---

<style>
    table{
        border: none;
        border-collapse: collapse;
        text-align: center;
        margin-bottom: 10px;
    }
    th, td {
        padding: 10px;
        text-align: center;
        border: 1px solid #ddd;
    }

    th {
        font-weight: bold;
        color: black;
    }

    td {
        background-color: #f9f9f9;
    }

    .percentage {
        color: green;
    }

    .percentage-negative {
        color: red;
    }
</style>

<div style="text-align: center;">
    <img src="/assets/post25/title.webp" alt="Nvidia RTX 5090" style="max-width: 50%; height: auto; display: inline-block;">
    <br><i>Nvidia RTX 5090. Official image from Nvidia.</i>
</div>


Nvidia RTX 5090 tests were performed on a system with an AMD Ryzen 9 9950X using the Nvidia's proprietary driver 570.86.16 and CUDA 12.8 in a Docker environment. Note that the driver is marked as 'beta', so it may be that GPU performance will differ with future releases. Hardware settings were default for all test cases, without hardware overclocking.

## GPUs

<table style="width:100%">
  <tr>
    <th>GPU</th>
    <th>Number of CUDA Cores</th>
    <th>Base Clock (MHz)</th>
    <th>Number of Tensor Cores</th>
    <th>VRAM (GB)</th>
    <th>VRAM Bandwidth (GB/s)</th>
    <th>Memory Bus Width (bits)</th>
    <th>TDP (W)</th>
    <th>Lithography (nm)</th>
    <th>Release Date</th>
  </tr>
  <tr>
    <th>GTX 1080 Ti</th>
    <td>3584</td>
    <td>1480</td>
    <td>-</td>
    <td>11</td>
    <td>484</td>
    <td>352</td>
    <td>250</td>
    <td>16</td>
    <td>Mar 2017</td>
  </tr>
  <tr>
    <th>RTX 2080 Ti</th>
    <td>4352</td>
    <td>1350</td>
    <td>544</td>
    <td>11</td>
    <td>616</td>
    <td>352</td>
    <td>250</td>
    <td>12</td>
    <td>Sep 2018</td>
  </tr>
  <tr>
    <th>RTX 3090</th>
    <td>10496</td>
    <td>1395</td>
    <td>328</td>
    <td>24</td>
    <td>936</td>
    <td>384</td>
    <td>350</td>
    <td>8</td>
    <td>Sep 2020</td>
  </tr>
  <tr>
    <th>RTX 4090</th>
    <td>16384</td>
    <td>2230</td>
    <td>512</td>
    <td>24</td>
    <td>1018</td>
    <td>384</td>
    <td>450</td>
    <td>5</td>
    <td>Sep 2022</td>
  </tr>
  <tr>
    <th>RTX 5090</th>
    <td>21760</td>
    <td>2017</td>
    <td>576</td>
    <td>32</td>
    <td>1792</td>
    <td>512</td>
    <td>575</td>
    <td>4</td>
    <td>Jan 2025</td>
  </tr>
</table>

Note that Tensor Cores were updated during each architecture update, adding support for different precisions and operations, as well as optimizations of these operations. Therefore, the Tensor Core count should not be considered a direct performance proxy metric.

## Computer Vision models

![Computer vision models benchmarks results](/assets/post25/timm.png)

The tests were performed using benchmarks from [timm](https://github.com/huggingface/pytorch-image-models), version 1.0.14, a collection of computer vision models. The selection of models is partially conditioned by previous benchmarks to provide some level of comparability with older results for previous generations of GPUs [\[1\]][1].
The benchmark was performed using nightly builds of PyTorch 2.6.0 with CUDA 12.8 support.

The set of results is based on a batch size of 256, which is most relevant to training scenarios and inference in concurrent applications. If the desired batch size does not fit into VRAM, it was reduced by steps of 32 until it fits. The image size for all models was set to 224x224. This can also be viewed as an upper boundary estimation of the GPUs throughput. At the same time, the tests are not meant to demonstrate the absolutely highest performance of the hardware, as advanced optimization techniques were not applied; instead, they attempt to compare different generations of video accelerators in roughly equal settings.

Note that these results do not include additional hardware-specific optimizations or `torch.compile` application, which are expected to change the results given different generations of Tensor Cores and differences between Tensor Cores subsystem features.

The reported increase percentage is calculated using RTX 3090 as the baseline. All results are in samples per second.

#### FP32 Comparison
<table>
    <thead>
        <tr>
            <th>GPU</th>
            <th colspan="2">vgg16</th>
            <th colspan="2">resnet50</th>
            <th colspan="2">tf_efficientnetv2_b0</th>
            <th colspan="2">swin_base_patch4_window7_224</th>
            <th colspan="2">efficientvit_m4</th>
        </tr>
        <tr>
            <th></th>
            <th>Inference</th>
            <th>Train</th>
            <th>Inference</th>
            <th>Train</th>
            <th>Inference</th>
            <th>Train</th>
            <th>Inference</th>
            <th>Train</th>
            <th>Inference</th>
            <th>Train</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>RTX 3090</td>
            <td>841.0</td>
            <td>260.8</td>
            <td>1679.9</td>
            <td>523.0</td>
            <td>4358.6</td>
            <td>1145.8</td>
            <td>493.9</td>
            <td>158.0</td>
            <td>10600.6</td>
            <td>2730.0</td>
        </tr>
        <tr>
            <td>RTX 4090</td>
            <td>1454.6 <span class="percentage">(+73.0%)</span></td>
            <td>456.5 <span class="percentage">(+75.1%)</span></td>
            <td>2433.1 <span class="percentage">(+44.8%)</span></td>
            <td>757.5 <span class="percentage">(+44.8%)</span></td>
            <td>6477.3 <span class="percentage">(+48.6%)</span></td>
            <td>1643.8 <span class="percentage">(+43.5%)</span></td>
            <td>855.3 <span class="percentage">(+73.2%)</span></td>
            <td>293.2 <span class="percentage">(+85.6%)</span></td>
            <td>18975.9 <span class="percentage">(+79.0%)</span></td>
            <td>3866.7 <span class="percentage">(+41.6%)</span></td>
        </tr>
        <tr>
            <td>RTX 5090</td>
            <td>1867.5 <span class="percentage">(+122.1%)</span></td>
            <td>594.7 <span class="percentage">(+128.1%)</span></td>
            <td>3576.8 <span class="percentage">(+112.9%)</span></td>
            <td>1128.6 <span class="percentage">(+115.8%)</span></td>
            <td>9254.5 <span class="percentage">(+112.3%)</span></td>
            <td>2448.9 <span class="percentage">(+113.7%)</span></td>
            <td>1315.8 <span class="percentage">(+166.4%)</span></td>
            <td>450.2 <span class="percentage">(+185.0%)</span></td>
            <td>23555.6 <span class="percentage">(+122.2%)</span></td>
            <td>6940.8 <span class="percentage">(+154.3%)</span></td>
        </tr>
    </tbody>
</table>

#### FP16 Comparison
<table>
    <thead>
        <tr>
            <th>GPU</th>
            <th colspan="2">vgg16</th>
            <th colspan="2">resnet50</th>
            <th colspan="2">tf_efficientnetv2_b0</th>
            <th colspan="2">swin_base_patch4_window7_224</th>
            <th colspan="2">efficientvit_m4</th>
        </tr>
        <tr>
            <th></th>
            <th>Inference</th>
            <th>Train</th>
            <th>Inference</th>
            <th>Train</th>
            <th>Inference</th>
            <th>Train</th>
            <th>Inference</th>
            <th>Train</th>
            <th>Inference</th>
            <th>Train</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>RTX 3090</td>
            <td>1387.6</td>
            <td>438.2</td>
            <td>2973.1</td>
            <td>888.7</td>
            <td>7010.4</td>
            <td>1818.3</td>
            <td>979.1</td>
            <td>337.0</td>
            <td>11087.8</td>
            <td>3114.9</td>
        </tr>
        <tr>
            <td>RTX 4090</td>
            <td>2418.6 <span class="percentage">(+74.3%)</span></td>
            <td>837.5 <span class="percentage">(+91.1%)</span></td>
            <td>4601.8 <span class="percentage">(+54.8%)</span></td>
            <td>1360.6 <span class="percentage">(+53.1%)</span></td>
            <td>12393.6 <span class="percentage">(+76.8%)</span></td>
            <td>2823.5 <span class="percentage">(+55.3%)</span></td>
            <td>1762.2 <span class="percentage">(+80.0%)</span></td>
            <td>597.1 <span class="percentage">(+77.2%)</span></td>
            <td>17223.6 <span class="percentage">(+55.3%)</span></td>
            <td>3810.7 <span class="percentage">(+22.3%)</span></td>
        </tr>
        <tr>
            <td>RTX 5090</td>
            <td>3350.1 <span class="percentage">(+141.4%)</span></td>
            <td>1161.0 <span class="percentage">(+164.9%)</span></td>
            <td>5741.6 <span class="percentage">(+93.1%)</span></td>
            <td>1623.9 <span class="percentage">(+82.7%)</span></td>
            <td>15907.3 <span class="percentage">(+126.9%)</span></td>
            <td>3446.1 <span class="percentage">(+89.5%)</span></td>
            <td>2471.9 <span class="percentage">(+152.5%)</span></td>
            <td>822.3 <span class="percentage">(+144.1%)</span></td>
            <td>31682.2 <span class="percentage">(+185.7%)</span></td>
            <td>7310.4 <span class="percentage">(+134.7%)</span></td>
        </tr>
    </tbody>
</table>

On average, we have about an equal boost of **132%** for both precisions by switching from Ampere to Blackwell (or **44%** for switching from Ada Lovelace to Blackwell). As just a speculation, a notable feature is that the boost is less significant (113 and 98% for FP32 and FP16 of RTX 5090 vs RTX 3090) if we consider convolutional-dominant models (ResNet and EfficientNet in the test), which may indicate that the newer GPU's architecture is more optimized for matrix multiplication dominant models, or the models benefit more from the update of the memory subsystem. Among these models (VGG and Swin Transformers), we can see a more significant boost for FP16, which is not surprising given modern training pipelines are often optimized for half-precision. Despite the test not providing facts to support the hypothesis, given the very fast nature of EfficientViT model, the model may see a more significant impact from VRAM bandwidth, which could be an explanation for the outlier results for the model.

### LLMs

![Ollama models benchmarks results](/assets/post25/ollama.png)

All tests were performed using Ollama 0.5.11 with an 8k context length and Q4_K_M quantisation, which is the default recommended quantisation level for Ollama.

All results are reported in tokens per second. The increase percentage is calculated using RTX 3090 as the baseline.

| Model               | RTX 3090 | RTX 4090 (Increase %) | RTX 5090 (Increase %) |
|---------------------|----------|-----------------------|-----------------------|
| deepseek-r1:32b     | 30.85    | 37.44 <span style="color:green;">(+21.36%)</span>       | 60.66 <span style="color:green;">(+96.63%)</span>       |
| qwen2.5:32b         | 32.12    | 38.15 <span style="color:green;">(+18.78%)</span>       | 62.81 <span style="color:green;">(+95.54%)</span>       |
| qwen2.5:7b          | 100.32   | 119.56 <span style="color:green;">(+19.18%)</span>      | 213.48 <span style="color:green;">(+112.80%)</span>     |
| mistral-small:24b   | 45.78    | 54.04 <span style="color:green;">(+17.99%)</span>       | 91.29 <span style="color:green;">(+99.37%)</span>       |
| phi4:14b            | 64.40    | 77.84 <span style="color:green;">(+20.87%)</span>       | 130.31 <span style="color:green;">(+102.35%)</span>     |
| phi3.5:3.8b         | 170.24   | 217.32 <span style="color:green;">(+27.69%)</span>      | 346.65 <span style="color:green;">(+103.62%)</span>     |
| llama3.1:8b         | 100.53   | 121.74 <span style="color:green;">(+21.10%)</span>      | 210.79 <span style="color:green;">(+109.68%)</span>     |
| llama3.2:3b         | 152.83   | 182.11 <span style="color:green;">(+19.24%)</span>      | 339.51 <span style="color:green;">(+122.33%)</span>     |
| qwen2.5:1.5b        | 170.29   | 214.98 <span style="color:green;">(+26.26%)</span>      | 402.32 <span style="color:green;">(+136.26%)</span>     |

Interestingly enough, average performance improvements of RTX 4090 vs RTX 3090 are less than those observed for Computer Vision models, which may be related to a more significant influence of memory bandwidth on language models or other features of the test setup or the models themselves.

On average, RTX 4090 outperforms RTX 3090 by about **21.4%**, while the latest gen GPU (RTX 5090) is faster than RTX 4090 by **72%**, which is a significant improvement between generations and may justify an update. The observed difference may be attributed to the fact that language models are more demanding on memory bandwidth and the latest generation's VRAM offers substantial (~1.7x) improvement over previous generations.

## Conclusion

To sum up, the generational gap between RTX 4090 and RTX 5090 is about **44%** in Computer Vision tasks and about **72%** in Natural Language Processing tasks, achieved at the cost of a ~28%  increase in power usage. In addition, transitioning to Blackwell offers faster and larger VRAM, which may provide further benefits for many applications. At the same time, upgrading from RTX 3090 generally more than doubles performance across all task types (**~132%** boost in Computer Vision and about **~108%** on average in Ollama LLMs inference). Of course, whether this upgrade is worthwhile depends on individual or organisational needs, desired features (considering the VRAM upgrade), and budget constraints.

The main question we still have to answer: Is Moore's law dead or not? We can consider a simplified formulation as doubling of computational performance every two years. If we compare the performance of the most recent GPU with Nvidia GTX 1080 Ti - the oldest one tested in this blog post [\[1\]][1] - we can see an FP16 training improvement of about 14.4x (for the Swin model). Given the duration between releases of GTX 1080 Ti and RTX 5090, we should expect a roughly 15x fold increase in compute. This suggests that mankind's progress in semiconductors is still near holding Moore's law, with the caveat that it may not be valid for FP32 compute or convolution-based models.
The Nvidia RTX 5090 Founders Edition GPU's convenient two-slot design makes it an excellent solution for dual-GPU workstations. With its notable TDP, when paired with a decent CPU, such a setup is not only a desired tool for many Deep Learning developers but also can double up as an efficient home heater during those chilly winter months.

References:

[1]: {% post_url 2022-11-30-Four-gens-of-Nvidia-GPUs-compared %}
\[1\]: [Four gens of Nvidia GPUs compared]({% post_url 2022-11-30-Four-gens-of-Nvidia-GPUs-compared %})







